{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPwXlgC/WDsqHX7eEvfen1C",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AzizKna/Naive-Bayes-Assignment/blob/main/Naive_Bayes_Aziz_Abbas_review_580953.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Created on Mon Mar  6 13:27:07 2023\n",
        "@author: casperbernards\n",
        "@reviewer: Aziz Abbas\n",
        "\"\"\"\n",
        "##AZIZ EDIT: Set Seed to have reproducible results\n",
        "import random\n",
        "\n",
        "# Set a seed value\n",
        "random.seed(42)\n",
        "\n",
        "# Import the relevant packages\n",
        "import pandas as pd\n",
        "from pandas.api.types import CategoricalDtype\n",
        "from IPython.display import display, Markdown\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "\n",
        "\n",
        "# id: unique id for a news article\n",
        "# title: the title of a news article\n",
        "# author: author of the news article\n",
        "# text: the text of the article; could be incomplete\n",
        "# label: a label that marks the article as potentially unreliable\n",
        "# 0: reliable\n",
        "# 1: unreliable\n",
        "\n",
        "\n",
        "\n",
        "# Import and print the dataset - Loading the dataset might take some time because of the internet connection\n",
        "#AZIZ EDIT: change \"read.csv\" to \"read_csv\", old code >> rawDF = pd.read.csv ('https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/NB-fakenews.csv')\n",
        "rawDF = pd.read_csv('https://raw.githubusercontent.com/HAN-M3DM-Data-Mining/assignments/master/datasets/NB-fakenews.csv')\n",
        "rawDF\n",
        "\n",
        "\n",
        "\n",
        "# print the column names\n",
        "print(rawDF.columns)\n",
        "# Drop unnecessary columns\n",
        "cleanDF = rawDF.drop (['id', 'title','author'], axis = 1)\n",
        "\n",
        "# define a dictionary to map label values to categories\n",
        "label_map = {0: 'reliable', 1: 'unreliable'}\n",
        "\n",
        "# create a new DataFrame with updated 'categorie' column\n",
        "##AZIZ EDIT: change \"type\" to \"label\", old code >> EDIT:cleanDF2 = cleanDF.assign(category=cleanDF['type'].map(label_map))\n",
        "cleanDF2 = cleanDF.assign(category=cleanDF['label'].map(label_map))\n",
        "\n",
        "\n",
        "# print the updated DataFrame\n",
        "print(cleanDF2)\n",
        "\n",
        "\n",
        "\n",
        "# Categorise the data as \"reliable\" or \"unreliable\" and print this outcome\n",
        "##AZIZ EDIT: spelling error, change \"unrrliable\" to \"unreliable\", old code >> catType = CategoricalDtype(categories=[\"reliable\", \"unrrliable\"], ordered=False)\n",
        "catType = CategoricalDtype(categories=[\"reliable\", \"unreliable\"], ordered=False)\n",
        "cleanDF2.category = cleanDF2.category.astype(catType)\n",
        "cleanDF2.category\n",
        "\n",
        "\n",
        "# Print the distribution of the messages using count\n",
        "cleanDF2.category.value_counts()\n",
        "# Print the same distribution but using relative numbers\n",
        "cleanDF2.category.value_counts(normalize=True)\n",
        "\n",
        "\n",
        "\n",
        "# Generate a word cloud image\n",
        "## Note to teacher: used str to convert text\n",
        "reliableText = ' '.join([str(Text) for Text in cleanDF2[cleanDF2['category']=='reliable']['text'].astype(str)])\n",
        "##AZIZ EDIT: spelling error, change \"catagory\" to \"category\", old code >> unreliableText = ' '.join([str(Text) for Text in cleanDF2[cleanDF2['catagory']=='unreliable']['text'].astype(str)])\n",
        "unreliableText = ' '.join([str(Text) for Text in cleanDF2[cleanDF2['category']=='unreliable']['text'].astype(str)])\n",
        "colorListReliable=['#e9f6fb','#92d2ed','#2195c5']\n",
        "colorListUnreliable=['#f9ebeb','#d57676','#b03636']\n",
        "colormapReliable=colors.ListedColormap(colorListReliable)\n",
        "colormapUnreliable=colors.ListedColormap(colorListUnreliable)\n",
        "wordcloudReliable = WordCloud(background_color='white', colormap=colormapReliable).generate(reliableText)\n",
        "wordcloudUnreliable = WordCloud(background_color='white', colormap=colormapUnreliable).generate(unreliableText)\n",
        "\n",
        "# Display the generated image (run all the lines below at once):\n",
        "fig, (wc1, wc2) = plt.subplots(1, 2)\n",
        "fig.suptitle('Wordclouds for reliable and unreliable')\n",
        "wc1.imshow(wordcloudReliable)\n",
        "wc2.imshow(wordcloudUnreliable)\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### --------- Training Model\n",
        "\n",
        "# Remove the rows that contain NaN values \n",
        "## Note to teacher: dropped the nan values\n",
        "rawDF.dropna(subset=['text'], inplace=True)\n",
        "\n",
        "# Convert text data into a numerical vector \n",
        "vectorizer = TfidfVectorizer(max_features=1000)\n",
        "vectors = vectorizer.fit_transform(rawDF.text)\n",
        "wordsDF = pd.DataFrame(vectors.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "wordsDF.head()\n",
        "\n",
        "# Split the dataset in four sets \n",
        "##AZIZ EDIT: change \"rawDF_label\" to \"rawDF.label\",xTrain, xTest, yTrain, yTest = train_test_split(wordsDF, rawDF_label)\n",
        "xTrain, xTest, yTrain, yTest = train_test_split(wordsDF, rawDF.label)\n",
        "\n",
        "\n",
        "\n",
        "# use algorithm to train a model on a dataset to predict new data\n",
        "bayes = MultinomialNB()\n",
        "bayes.fit(xTrain, yTrain)\n",
        "\n",
        "# use trained model (bayes) to predict the labels of the test dataset and compare the predicted outcome\n",
        "yPred = bayes.predict(xTest)\n",
        "yTrue = yTest\n",
        "\n",
        "# calculate the accuracy of a classification model, print the score, generate a confusion matrix based on the prediction and display the matrix\n",
        "accuracyScore = accuracy_score(yTrue, yPred)\n",
        "print(f'Accuracy: {accuracyScore}')\n",
        "matrix = confusion_matrix(yTrue, yPred)\n",
        "labelNames = pd.Series(['reliable', 'unreliable'])\n",
        "pd.DataFrame(matrix,\n",
        "     columns='Predicted ' + labelNames,\n",
        "     index='Is ' + labelNames)\n",
        "\n",
        "##AZIZ EDIT: more explanation of the results I got. However, results are not reproducible.\n",
        "## Naive Bayes classification model was used to predict the reliability of news articles. \n",
        "## The data consists of five columns: 'id', 'title', 'author', 'text', and 'label'. \n",
        "## The 'text' column contains the content of the news articles, while the 'label' column denotes whether the article is reliable (0) or unreliable (1).\n",
        "## The model achieved an accuracy of 0.8707, which means that it correctly classified 87.07% of the articles in the dataset. \n",
        "## The confusion matrix shows the number of articles that were correctly or incorrectly classified by the model.\n",
        "## The rows represent the actual labels of the articles, while the columns represent the predicted labels.\n",
        "## According to the confusion matrix, out of 3,171 articles, the model predicted 2,580 to be reliable and 591 to be unreliable. \n",
        "## Of the articles that were actually reliable, the model correctly predicted 2,346 and incorrectly predicted 253 as unreliable. \n",
        "## Of the articles that were actually unreliable, the model correctly predicted 2,174 and incorrectly predicted 418 as reliable.\n",
        "\n",
        "##In order to have reproducble results, you can use the following code:\n",
        "##change the code starting from #split the data in four sets to\n",
        "###Split the data into training and testing sets\n",
        "##X_train, X_test, y_train, y_test = train_test_split(vectors, rawDF['label'], test_size=0.2, random_state=42)\n",
        "\n",
        "#Train the Naive Bayes classifier on the training data\n",
        "##clf = MultinomialNB()\n",
        "##clf.fit(X_train, y_train)\n",
        "\n",
        "#Use the trained classifier to predict the labels for the test data\n",
        "##y_pred = clf.predict(X_test)\n",
        "\n",
        "#Evaluate the performance of the classifier using confusion matrix and accuracy score\n",
        "##cm = confusion_matrix(y_test, y_pred)\n",
        "##accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "#Print the confusion matrix and accuracy score\n",
        "##print(\"Confusion Matrix:\\n\", cm)\n",
        "##print(\"Accuracy Score:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "IVpsrjlaJHso",
        "outputId": "f411da4e-0cbe-4f7d-ef7e-c86a7e39a16b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'title', 'author', 'text', 'label'], dtype='object')\n",
            "                                                    text  label    category\n",
            "0      House Dem Aide: We Didn’t Even See Comey’s Let...      1  unreliable\n",
            "1      Ever get the feeling your life circles the rou...      0    reliable\n",
            "2      Why the Truth Might Get You Fired October 29, ...      1  unreliable\n",
            "3      Videos 15 Civilians Killed In Single US Airstr...      1  unreliable\n",
            "4      Print \\nAn Iranian woman has been sentenced to...      1  unreliable\n",
            "...                                                  ...    ...         ...\n",
            "20795  Rapper T. I. unloaded on black celebrities who...      0    reliable\n",
            "20796  When the Green Bay Packers lost to the Washing...      0    reliable\n",
            "20797  The Macy’s of today grew from the union of sev...      0    reliable\n",
            "20798  NATO, Russia To Hold Parallel Exercises In Bal...      1  unreliable\n",
            "20799    David Swanson is an author, activist, journa...      1  unreliable\n",
            "\n",
            "[20800 rows x 3 columns]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-749f2e6dc31d>\u001b[0m in \u001b[0;36m<cell line: 83>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0mcolormapReliable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListedColormap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolorListReliable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0mcolormapUnreliable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mListedColormap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolorListUnreliable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m \u001b[0mwordcloudReliable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolormapReliable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreliableText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0mwordcloudUnreliable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbackground_color\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'white'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolormap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolormapUnreliable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munreliableText\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    637\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    638\u001b[0m         \"\"\"\n\u001b[0;32m--> 639\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    640\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    618\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m         \"\"\"\n\u001b[0;32m--> 620\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/wordcloud/wordcloud.py\u001b[0m in \u001b[0;36mprocess_text\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    580\u001b[0m         \u001b[0mregexp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregexp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mpattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 582\u001b[0;31m         \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mregexp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    583\u001b[0m         \u001b[0;31m# remove 's\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         words = [word[:-2] if word.lower().endswith(\"'s\") else word\n",
            "\u001b[0;32m/usr/lib/python3.9/re.py\u001b[0m in \u001b[0;36mfindall\u001b[0;34m(pattern, string, flags)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    240\u001b[0m     Empty matches are included in the result.\"\"\"\n\u001b[0;32m--> 241\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfinditer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YnQ1SSMX6Xuv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}